import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from tqdm import tqdm
import os
import random
import numpy as np

# ëª¨ë“ˆë“¤ ë¶ˆëŸ¬ì˜¤ê¸°
from src.dataset import DeepFakeDataset
from src.model import DeepFakeModel

# ====================================================
# í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •
# ====================================================
CONFIG = {
    'model_name': 'efficientnet_b4',  
    'batch_size': 8,                  
    'epochs': 10,
    'lr': 1e-4,
    'seed': 42,
    'save_path': './model/best_model.pth' 
}

# ====================================================
#  ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# ====================================================
def seed_everything(seed):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = True

def train_one_epoch(model, loader, criterion, optimizer, scaler, device):
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0
    
    pbar = tqdm(loader, desc="Training")
    for images, labels in pbar:
        images = images.to(device)
        labels = labels.to(device).float().unsqueeze(1)
        
        optimizer.zero_grad()
        
        with torch.cuda.amp.autocast():
            outputs = model(images)
            loss = criterion(outputs, labels)
            
        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()
        
        running_loss += loss.item()
        probs = torch.sigmoid(outputs)
        preds = (probs > 0.5).float()
        
        total += labels.size(0)
        correct += (preds == labels).sum().item()
        
        pbar.set_postfix({'loss': running_loss / (pbar.n + 1)})
        
    epoch_loss = running_loss / len(loader)
    epoch_acc = correct / total * 100
    return epoch_loss, epoch_acc

def validate(model, loader, criterion, device):
    model.eval()
    running_loss = 0.0
    correct = 0
    total = 0
    
    with torch.no_grad():
        for images, labels in tqdm(loader, desc="Validation"):
            images = images.to(device)
            labels = labels.to(device).float().unsqueeze(1)
            
            outputs = model(images)
            loss = criterion(outputs, labels)
            
            running_loss += loss.item()
            probs = torch.sigmoid(outputs)
            preds = (probs > 0.5).float()
            
            total += labels.size(0)
            correct += (preds == labels).sum().item()
            
    epoch_loss = running_loss / len(loader)
    epoch_acc = correct / total * 100
    return epoch_loss, epoch_acc

# ====================================================
# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
# ====================================================
def main():
    seed_everything(CONFIG['seed'])
    
    save_dir = os.path.dirname(CONFIG['save_path']) # './model' ì¶”ì¶œ
    if not os.path.exists(save_dir):
        os.makedirs(save_dir)
        print(f"ğŸ“‚ '{save_dir}' í´ë”ë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤.")

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ”¥ ì‚¬ìš© ì¥ì¹˜: {device}")
    
    # 1. ë°ì´í„°ì…‹ & ë¡œë” ì¤€ë¹„
    print("ğŸ’¿ ë°ì´í„° ë¡œë”© ì¤‘...")

    sample_ratio = 1.0
    
    train_dataset = DeepFakeDataset(root_dir='./train_data', mode='train', sample_ratio=sample_ratio)
    
    if os.path.exists('./val_data'):
        val_dataset = DeepFakeDataset(root_dir='./val_data', mode='val', sample_ratio=sample_ratio)
    else:
        print("âš ï¸ ì£¼ì˜: val_data í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤. train_dataë¥¼ ê²€ì¦ìš©ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        val_dataset = DeepFakeDataset(root_dir='./train_data', mode='val', sample_ratio=sample_ratio)

    train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=True, num_workers=4, pin_memory=True)
    val_loader = DataLoader(val_dataset, batch_size=CONFIG['batch_size'], shuffle=False, num_workers=4, pin_memory=True)
    
    # 2. ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
    print(f"ğŸ¤– ëª¨ë¸ ë¡œë“œ ì¤‘: {CONFIG['model_name']}...")
    model = DeepFakeModel(model_name=CONFIG['model_name'], pretrained=True).to(device)
    
    # 3. ì„¤ì •
    criterion = nn.BCEWithLogitsLoss()
    optimizer = optim.AdamW(model.parameters(), lr=CONFIG['lr'], weight_decay=1e-2)
    scaler = torch.cuda.amp.GradScaler()
    
    # 4. í•™ìŠµ ì‹œì‘
    best_acc = 0.0
    print("\nğŸš€ í•™ìŠµ ì‹œì‘!")
    
    for epoch in range(CONFIG['epochs']):
        print(f"\nğŸ“¢ Epoch {epoch+1}/{CONFIG['epochs']}")
        
        train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, scaler, device)
        print(f"   [Train] Loss: {train_loss:.4f} | Acc: {train_acc:.2f}%")
        
        val_loss, val_acc = validate(model, val_loader, criterion, device)
        print(f"   [Valid] Loss: {val_loss:.4f} | Acc: {val_acc:.2f}%")
        
        if val_acc > best_acc:
            print(f"   ğŸ‰ ìµœê³  ì„±ëŠ¥ ê°±ì‹ ! ({best_acc:.2f}% -> {val_acc:.2f}%) './model' í´ë”ì— ì €ì¥ ì¤‘...")
            best_acc = val_acc
            torch.save(model.state_dict(), CONFIG['save_path'])
            
    print(f"\nğŸ í•™ìŠµ ì™„ë£Œ! ìµœê³  ì •í™•ë„: {best_acc:.2f}%")
    print(f"ğŸ’¾ ëª¨ë¸ ì €ì¥ ìœ„ì¹˜: {CONFIG['save_path']}")

if __name__ == "__main__":
    main()